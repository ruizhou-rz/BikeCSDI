{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd /content/drive/MyDrive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v0gDTFheBIjs","outputId":"2b71d222-50ee-46bd-bdf6-19d06fa8d128","executionInfo":{"status":"ok","timestamp":1702291162054,"user_tz":-480,"elapsed":21250,"user":{"displayName":"Jise Shen","userId":"07750767390677850627"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kapJvuRJ6EEL","outputId":"4fbf58c0-bdd3-481b-af4d-e8ed15ad85e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'TabCSDI' already exists and is not an empty directory.\n","/content/drive/MyDrive/TabCSDI\n","Collecting category_encoders==2.5.1.post0 (from -r requirements.txt (line 1))\n","  Downloading category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m695.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\n","Collecting pandas==1.5.2 (from -r requirements.txt (line 3))\n","  Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyYAML==6.0 (from -r requirements.txt (line 4))\n","  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting setuptools==61.2.0 (from -r requirements.txt (line 5))\n","  Downloading setuptools-61.2.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch==1.13.0 (from -r requirements.txt (line 6))\n","  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm==4.64.0 (from -r requirements.txt (line 7))\n","  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (1.2.2)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (1.11.3)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (0.14.0)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (0.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2->-r requirements.txt (line 3)) (2023.3.post1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->-r requirements.txt (line 6)) (4.5.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0->-r requirements.txt (line 6))\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0->-r requirements.txt (line 6))\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0->-r requirements.txt (line 6))\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0->-r requirements.txt (line 6))\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->-r requirements.txt (line 6)) (0.41.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (3.2.0)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders==2.5.1.post0->-r requirements.txt (line 1)) (23.2)\n","Installing collected packages: tqdm, setuptools, PyYAML, nvidia-cuda-nvrtc-cu11, pandas, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, category_encoders\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.66.1\n","    Uninstalling tqdm-4.66.1:\n","      Successfully uninstalled tqdm-4.66.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 67.7.2\n","    Uninstalling setuptools-67.7.2:\n","      Successfully uninstalled setuptools-67.7.2\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 6.0.1\n","    Uninstalling PyYAML-6.0.1:\n","      Successfully uninstalled PyYAML-6.0.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu118\n","    Uninstalling torch-2.1.0+cu118:\n","      Successfully uninstalled torch-2.1.0+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires openai, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\n","cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 61.2.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.5.2 which is incompatible.\n","torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n","torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed PyYAML-6.0 category_encoders-2.5.1.post0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.5.2 setuptools-61.2.0 torch-1.13.0 tqdm-4.64.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_distutils_hack","pkg_resources","setuptools"]}}},"metadata":{}}],"source":["!git clone https://github.com/UrsaTechnology/ControlNet-Projects.git\n","%cd /ControlNet-Projects/TabCSDI\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","source":["**Train**\n","\n","Before the training, please upload the dataset csv to the path \"TabCSDI\""],"metadata":{"id":"1GXo3TgDeG9C"}},{"cell_type":"code","source":["main.py --nfold 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urmF7iIzHuvS","outputId":"5d51e9b4-fa6f-414a-ec16-c1132d46baa3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(config='config.yaml', device='cuda', seed=3047, testmissingratio=0.2, nfold=100, unconditional=0, modelfolder='100_20231029_053837', nsample=50)\n","{\n","    \"train\": {\n","        \"epochs\": 10,\n","        \"batch_size\": 50,\n","        \"lr\": 0.001\n","    },\n","    \"diffusion\": {\n","        \"layers\": 4,\n","        \"channels\": 128,\n","        \"nheads\": 4,\n","        \"diffusion_embedding_dim\": 128,\n","        \"beta_start\": 0.0001,\n","        \"beta_end\": 0.5,\n","        \"num_steps\": 100,\n","        \"schedule\": \"quad\",\n","        \"mixed\": true,\n","        \"token_emb_dim\": 8\n","    },\n","    \"model\": {\n","        \"is_unconditional\": 0,\n","        \"timeemb\": 128,\n","        \"featureemb\": 16,\n","        \"target_strategy\": \"random\",\n","        \"mixed\": true,\n","        \"token_emb_dim\": 8,\n","        \"test_missing_ratio\": 0.2\n","    }\n","}\n","model folder: ./save/100_20231029_062736/\n","--------Normalized dataset loaded--------\n","Dataset size:124511 entries\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","Training dataset size: 123265\n","Validation dataset size: 0\n","Testing dataset size: 1246\n","---------------Start testing---------------\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149] [2, 2, 5, 1, 2, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n","100% 25/25 [54:54<00:00, 131.78s/it, rmse_total=0.0529, batch_no=25]\n","RMSE: 0.05287392437458038\n","ERR_CATE: [0.35802469 0.04081633 0.         0.         0.37596899 0.27799228\n"," 0.34163701 0.43983402 0.43359375 0.46413502 0.10833333 0.1015625\n"," 0.20833333 0.38636364 0.12295082 0.00423729 0.06302521 0.03968254\n"," 0.02592593 0.07581227 0.02834008 0.22821577 0.1755102  0.01136364\n"," 0.41984733 0.03937008 0.22891566 0.06177606 0.04545455 0.0984252\n"," 0.0244898  0.07421875 0.50220264 0.508      0.40248963 0.02631579\n"," 0.12107623 0.22672065 0.06382979 0.0251046  0.51449275 0.38113208\n"," 0.07434944 0.13452915 0.20472441 0.04065041 0.23236515 0.13090909\n"," 0.08424908 0.06198347 0.41666667 0.33333333 0.15322581 0.13888889\n"," 0.07434944 0.10084034 0.19921875 0.1733871  0.01492537 0.02380952\n"," 0.06837607 0.04958678 0.00829876 0.01339286 0.01321586 0.00421941\n"," 0.00796813 0.004329   0.01581028 0.00840336 0.19083969 0.05078125\n"," 0.11297071 0.02631579]\n"]}]},{"cell_type":"markdown","source":["**Distribution Exploration**"],"metadata":{"id":"_oNM4iswQRS_"}},{"cell_type":"code","source":["from src.utils_table import *\n","from src.main_model_table_ft import *\n","from data import *\n","import torch\n","import datetime\n","import json\n","import yaml\n","import os\n","import matplotlib.pyplot as plt\n","\n","indices = [10, 119, 160]\n","path = './save/100_20231029_053837/model.pth'  # Check the path \"save\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","with open(\"./config/config.yaml\", \"r\") as f:\n","    config = yaml.safe_load(f)\n","\n","\n","config[\"model\"][\"is_unconditional\"] = False\n","config[\"model\"][\"test_missing_ratio\"] = 0.2\n","\n","model = TabCSDI(config, device).to(device)\n","\n","model.load_state_dict(torch.load(path))\n","\n","_, _, test_loader = get_dataloader(seed=3047, batch_size=1, nfold=100)\n","for i in indices:\n","  test_loader.dataset.gt_masks[:, i] = False\n","\n","samples = []\n","\n","for i in tqdm(range(10)):\n","  sample, _, _, _, _ = model.evaluate(next(iter(test_loader)), 100)\n","  sample = sample.permute(0, 1, 3, 2)\n","  samples.append(sample)"],"metadata":{"id":"ZGbYBrLnYvnO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98a950c2-a98f-4027-ee6b-b085bfcf7596"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100\n","100\n","--------Normalized dataset loaded--------\n","Dataset size:124511 entries\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","Training dataset size: 123265\n","Validation dataset size: 0\n","Testing dataset size: 1246\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [12:36<00:00, 75.65s/it]\n"]}]},{"cell_type":"code","source":["with open(\"./data/transformed_columns.pk\", \"rb\") as f:\n","    cont_list, num_cate_list = pickle.load(f)\n","with open(\"./data/encoder.pk\", \"rb\") as f:\n","        encoder = pickle.load(f)\n","\n","decoded_samples = []\n","\n","for sample in samples:\n","  temp = torch.zeros(100,224)\n","  for i in range(100):\n","    temp[i,:] = model.tokenizer.recover(sample[:,i,:,:], len(cont_list))[0]  # Recover category from embedding\n","  decoded_samples.append(temp)"],"metadata":{"id":"clJnE3vtuE2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","for i in range(10):\n","  for ind in [10, 119, 160]:\n","    plt.hist(np.array(decoded_samples[i][:,ind]),bins=10)\n","    plt.savefig(\"{},{}.png\".format(i, ind), bbox_inches='tight')\n","    plt.close()"],"metadata":{"id":"57kI_LU2QSBK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for ind in [10, 119, 160]:\n","  plt.hist([np.array(decoded_samples[i][:,ind]) for i in range(10)],bins=20,stacked=True)\n","  plt.savefig(\"{}.png\".format(ind), bbox_inches='tight')\n","  plt.close()"],"metadata":{"id":"8qljc-9ySSY7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Single Feature Imputation**"],"metadata":{"id":"jjxQd-XaQXcJ"}},{"cell_type":"code","source":["from src.utils_table import *\n","from src.main_model_table_ft import *\n","from data import *\n","import torch\n","import datetime\n","import json\n","import yaml\n","import os\n","\n","indices = [10, 119, 160]\n","path = './save/100_20231029_053837/model.pth'  # Check the path \"save\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","with open(\"./config/config.yaml\", \"r\") as f:\n","    config = yaml.safe_load(f)\n","\n","\n","config[\"model\"][\"is_unconditional\"] = False\n","config[\"model\"][\"test_missing_ratio\"] = 0.2\n","\n","model = TabCSDI(config, device).to(device)\n","\n","model.load_state_dict(torch.load(path))\n","\n","_, _, test_loader = get_dataloader(seed=3047, batch_size=60, nfold=100, missing_ratio=0)\n","\n","for i in range(20):\n","  test_loader.dataset.gt_masks[i, 10] = False\n","for i in range(20,40):\n","  test_loader.dataset.gt_masks[i, 119] = False\n","for i in range(40,60):\n","  test_loader.dataset.gt_masks[i, 160] = False   # Manual mask\n","\n","with open(\"./column_index.pk\", \"rb\") as f:\n","  column_index = pickle.load(f)\n","\n","sample, _, _, _, _ = model.evaluate(next(iter(test_loader)), 5)\n","sample = sample.permute(0, 1, 3, 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"__eVDK_pQXAJ","outputId":"40b75d66-1552-47c4-c3b7-9d10b58d9683"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'BB textfield': 0, 'Seat tube length': 1, 'Stack': 2, 'Head tube upper extension2': 3, 'Seat angle': 4, 'CS textfield': 5, 'FCD textfield': 6, 'Seat tube extension2': 7, 'Head tube lower extension2': 8, 'Head angle': 9, 'Saddle height': 10, 'ERD rear': 11, 'Wheel width rear': 12, 'Dropout spacing': 13, 'SPOKES composite front': 14, 'SPOKES front': 15, 'Wheel diameter front': 16, 'BSD front': 17, 'Wheel diameter rear': 18, 'SPOKES rear': 19, 'Wheel width front': 20, 'SPOKES composite rear': 21, 'ERD front': 22, 'BSD rear': 23, 'SBLADEW front': 24, 'SBLADEW rear': 25, 'FORK0R': 26, 'FORK0L': 27, 'Saddle P': 28, 'Saddle thickness': 29, 'Saddle angle': 30, 'Saddle J': 31, 'Saddle H': 32, 'Saddle E': 33, 'SADDLETIPtoMIDDLE': 34, 'Saddle length': 35, 'Seatpost setback': 36, 'Seatpost LENGTH': 37, 'Lower stack height': 38, 'Upper stack height': 39, 'Headset spacers': 40, 'Stem angle': 41, 'CLAMPOFFSET': 42, 'StemX': 43, 'StemY': 44, 'StemG': 45, 'Collar height': 46, 'Stem length': 47, 'AEROF': 48, 'Hand horizontal': 49, 'Pad vertical': 50, 'Hand vertical': 51, 'MtnBar angle': 52, 'Mountain bar sweep': 53, 'MTNDROP': 54, 'HBARTHETA': 55, 'Mountain bar rise': 56, 'Handlebar angle': 57, 'Bullhorn reach': 58, 'BULLTHETA': 59, 'Road bar reach': 60, 'BULLRADIUS2': 61, 'Road bar drop': 62, 'Brake lever position': 63, 'Bullhorn drop': 64, 'Bullhorn rise': 65, 'Bullhorn angle': 66, 'HBAREXTEND': 67, 'HBARRAD1': 68, 'HBARRAD2': 69, 'Down tube front diameter': 70, 'Down tube rear dia2': 71, 'Down tube front dia2': 72, 'Down tube rear diameter': 73, 'Seat tube leading edge textfield': 74, 'BB length': 75, 'Seat tube chord length textfield': 76, 'Top tube rear diameter': 77, 'Down tube leading edge': 78, 'Head tube diameter': 79, 'DOWNTUBE Wheel cut': 80, 'Wheel cut': 81, 'Down tube aero diameter': 82, 'Top tube rear dia2': 83, 'Top tube front dia2': 84, 'BB diameter': 85, 'Head tube chord': 86, 'Seat tube diameter': 87, 'OFFSET_AT_BB st': 88, 'Top tube front diameter': 89, 'Head tube d': 90, 'Head tube B': 91, 'Head tube E': 92, 'Head tube D': 93, 'OFFSET_AT_BB dt': 94, 'CHAINSTAYOFFSET': 95, 'CHAINSTAYAUXrearDIAMETER': 96, 'Chain stay horizontal diameter': 97, 'Chain stay position on BB': 98, 'Chain stay taper': 99, 'Chain stay back diameter': 100, 'Chain stay vertical diameter': 101, 'CHAINSTAYbrdgdia1': 102, 'CHAINSTAYbrdgshift': 103, 'Seat stay junction0': 104, 'Seat stay bottom diameter': 105, 'SEATSTAY_HF': 106, 'SSTopZOFFSET': 107, 'SEATSTAY_HR': 108, 'SEATSTAYTAPERLENGTH': 109, 'SEATSTAYbrdgdia1': 110, 'SEATSTAYbrdgoffset': 111, 'bottle SEATTUBE0 WBX': 112, 'bottle SEATTUBE0 WBY': 113, 'bottle DOWNTUBE0 X': 114, 'bottle DOWNTUBE0 WBY': 115, 'bottle DOWNTUBE0 WBX': 116, 'bottle SEATTUBE0 X': 117, 'Front Fender clearance': 118, 'Front fender end angle': 119, 'Rear fender end angle': 120, 'Fender clearance': 121, 'Rear fender start angle': 122, 'Front fender start angle': 123, 'FDDIST': 124, 'SPIDER_DIAMETER': 125, 'Crank length': 126, 'Number of cogs': 127, 'Crank Q factor': 128, 'PEDAL_LENGTH': 129, 'Teeth on cog 2': 130, 'Teeth on cog 3': 131, 'Teeth on cog 4': 132, 'Teeth on cog 5': 133, 'Teeth on cog 6': 134, 'Teeth on cog 7': 135, 'Teeth on cog 8': 136, 'Teeth on cog 9': 137, 'Pedal width': 138, 'Teeth on cog 10': 139, 'Teeth on chainring 0': 140, 'Teeth on chainring 1': 141, 'Teeth on chainring 2': 142, 'CassZ': 143, 'CassT': 144, 'CassA': 145, 'Teeth on cog 11': 146, 'BACKGROUND color R_RGB': 147, 'BACKGROUND color G_RGB': 148, 'BACKGROUND color B_RGB': 149, 'SAME_SIZED_FRONT_AND_REAR': 150, 'Display AEROBARS': 151, 'HBARRISE': 152, 'Bullhorn risy': 153, 'CHAINSTAYbrdgCheck': 154, 'bottle SEATTUBE0 CAGE': 155, 'bottle DOWNTUBE0 CAGE': 156, 'Front Fender include': 157, 'Rear Fender include': 158, 'Number of chainrings': 159, 'Display RACK': 160, 'Dropout spacing style OHCLASS: 0': 161, 'Dropout spacing style OHCLASS: 1': 162, 'Dropout spacing style OHCLASS: 2': 163, 'Dropout spacing style OHCLASS: 3': 164, 'RIM_STYLE front OHCLASS: DISC': 165, 'RIM_STYLE front OHCLASS: SPOKED': 166, 'RIM_STYLE front OHCLASS: TRISPOKE': 167, 'RIM_STYLE rear OHCLASS: DISC': 168, 'RIM_STYLE rear OHCLASS: SPOKED': 169, 'RIM_STYLE rear OHCLASS: TRISPOKE': 170, 'Fork type OHCLASS: 0': 171, 'Fork type OHCLASS: 1': 172, 'Fork type OHCLASS: 2': 173, 'HEADSETprofile OHCLASS: 0': 174, 'HEADSETprofile OHCLASS: 1': 175, 'HEADSETprofile OHCLASS: 2': 176, 'HEADSETprofile OHCLASS: 3': 177, 'HEADSETprofile OHCLASS: 4': 178, 'Stem kind OHCLASS: 0': 179, 'Stem kind OHCLASS: 1': 180, 'Stem kind OHCLASS: 2': 181, 'Aerobar style OHCLASS: 0': 182, 'Aerobar style OHCLASS: 1': 183, 'Brake lever brand OHCLASS: CAMPAGNOLO': 184, 'Brake lever brand OHCLASS: CAMPAGNOLO_BAR_END': 185, 'Brake lever brand OHCLASS: SHIMANO': 186, 'Brake lever brand OHCLASS: SHIMANO_2012': 187, 'Brake lever brand OHCLASS: SRAM': 188, 'Brake lever brand OHCLASS: TEKTRO': 189, 'Handlebar style OHCLASS: 0': 190, 'Handlebar style OHCLASS: 1': 191, 'Handlebar style OHCLASS: 2': 192, 'TRACK_ERGO OHCLASS: 0': 193, 'TRACK_ERGO OHCLASS: 1': 194, 'Head tube type OHCLASS: 0': 195, 'Head tube type OHCLASS: 1': 196, 'Head tube type OHCLASS: 2': 197, 'Head tube type OHCLASS: 3': 198, 'Seat tube type OHCLASS: 0': 199, 'Seat tube type OHCLASS: 1': 200, 'Seat tube type OHCLASS: 2': 201, 'Top tube type OHCLASS: 0': 202, 'Top tube type OHCLASS: 1': 203, 'bottle SEATTUBE0 show OHCLASS: False': 204, 'bottle SEATTUBE0 show OHCLASS: True': 205, 'bottle DOWNTUBE0 show OHCLASS: False': 206, 'bottle DOWNTUBE0 show OHCLASS: True': 207, 'BELTorCHAIN OHCLASS: 0': 208, 'BELTorCHAIN OHCLASS: 1': 209, 'MOUNT_TYPE OHCLASS: BRAZEON': 210, 'MOUNT_TYPE OHCLASS: CLAMP': 211, 'MOUNT_TYPE OHCLASS: DIRECT_MOUNT': 212, 'BRAZEonFDTYPE OHCLASS: FD9000F': 213, 'BRAZEonFDTYPE OHCLASS: FD9070F': 214, 'DIRECTMOUNTFDTYPE OHCLASS: FDM771D': 215, 'DIRECTMOUNTFDTYPE OHCLASS: FDM9000D': 216, 'DIRECTMOUNTFDTYPE OHCLASS: FDM9025D': 217, 'CLAMPFDTYPE OHCLASS: FD9000B': 218, 'CLAMPFDTYPE OHCLASS: FDM9000L': 219, 'spc type OHCLASS: 0': 220, 'spc type OHCLASS: 1': 221, 'spc type OHCLASS: 2': 222, 'spc type OHCLASS: 3': 223}\n","--------Dataset created--------\n","Dataset size:124511 entries\n","--------------Dataset has not been normalized yet. Perform data normalization and store the mean value of each column.--------------\n","--------------Max-value for cont-variable column [6.0600e+02 9.4840e+02 1.5000e+03 3.5000e+02 1.2000e+02 2.0000e+03\n"," 2.3736e+03 6.1480e+02 2.9000e+02 9.0000e+01 1.7027e+03 1.6620e+03\n"," 2.0320e+02 2.5600e+02 1.0000e+01 2.0000e+01 1.7520e+03 1.7000e+03\n"," 1.7520e+03 2.0000e+01 2.0320e+02 1.0000e+01 1.6620e+03 1.7000e+03\n"," 5.0000e+02 1.4000e+02 7.8500e+02 9.0500e+02 1.1000e+02 8.0000e+01\n"," 3.5600e+02 1.0000e+02 6.5000e+01 6.4000e+01 2.2500e+02 8.3000e+02\n"," 1.0000e+02 7.0000e+02 4.4000e+01 1.0000e+02 1.2500e+02 3.4000e+02\n"," 2.9500e+02 1.8500e+02 6.5000e+02 1.5500e+02 1.6000e+02 4.0000e+02\n"," 1.3000e+02 4.0000e+02 7.0000e+02 1.3000e+02 3.5000e+02 6.0000e+02\n"," 1.2700e+02 2.0500e+02 4.0000e+02 9.1500e+02 6.0000e+02 2.2400e+02\n"," 2.0000e+02 1.5000e+02 2.5400e+02 1.0000e+02 2.1000e+02 3.0480e+02\n"," 2.0500e+02 1.5500e+02 9.0000e+01 1.0000e+03 1.3000e+02 1.3650e+02\n"," 1.3000e+02 1.3650e+02 5.0000e+01 7.0800e+03 4.0000e+02 1.5000e+02\n"," 4.5100e+01 8.0000e+01 7.1000e+02 8.4200e+02 1.2300e+02 1.5000e+02\n"," 1.5100e+02 9.0000e+01 2.0000e+02 1.4000e+02 5.0900e+02 1.3000e+02\n"," 8.0000e+01 2.0000e+02 8.0000e+01 7.0000e+01 1.5000e+03 4.2500e+02\n"," 3.0000e+02 6.6660e+03 8.8880e+03 6.0000e+02 1.1000e+02 1.1000e+02\n"," 5.8000e+01 5.5000e+02 4.7400e+02 1.3000e+02 2.5000e+02 8.0000e+01\n"," 9.3000e+01 9.0000e+02 5.4000e+01 2.5000e+01 3.5000e+01 1.8000e+01\n"," 4.5000e+02 1.8000e+01 6.5000e+01 4.5000e+02 1.0000e+02 2.2500e+02\n"," 2.2000e+02 1.0000e+02 9.0000e+01 1.3000e+02 2.0000e+02 4.5000e+02\n"," 2.7920e+02 1.1000e+01 6.0000e+02 1.5000e+02 7.5000e+01 4.0000e+01\n"," 5.0000e+01 6.0000e+01 7.2000e+01 8.0000e+01 9.0000e+01 1.0000e+02\n"," 2.2000e+02 1.1000e+02 1.9000e+02 7.0000e+01 4.4000e+01 4.0000e+01\n"," 1.1000e+01 2.4000e+01 5.1000e+01 3.3023e+04 2.5500e+02 2.5500e+02]--------------\n","--------------Min-value for cont-variable column [-1.0600e+03  0.0000e+00 -8.6600e+01 -3.0000e+01  5.0000e+00  0.0000e+00\n","  0.0000e+00 -1.2840e+02 -7.3830e+02  1.9000e+01  0.0000e+00  0.0000e+00\n","  0.0000e+00  1.0900e+02  0.0000e+00  0.0000e+00  1.2000e+02  1.0000e+02\n","  2.3000e+02  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  1.7000e+02\n","  5.0000e+00  5.0000e+00 -3.5000e+01  1.4150e+02 -1.0000e+02  0.0000e+00\n"," -5.2000e+01  2.0000e+01  1.0000e+01  1.0000e+01  6.0000e+01  1.3000e+02\n"," -9.9000e+01  0.0000e+00  0.0000e+00 -1.5500e+02 -1.6600e+02 -1.5800e+02\n"," -3.2000e+01 -5.8000e+02 -3.0000e+01 -9.7000e+01 -2.8000e+02 -8.5000e+02\n"," -4.0000e+01 -5.0000e+01 -3.0000e+01 -4.0000e+01 -1.8000e+02 -1.3000e+02\n"," -3.0000e+02 -2.3500e+01 -1.6000e+02 -2.0000e+02  0.0000e+00 -2.2000e+02\n"," -1.5000e+01 -1.1500e+02 -7.5000e+01  0.0000e+00 -1.3000e+02 -9.0000e+01\n"," -2.0500e+02 -1.5000e+01 -3.5000e+01 -3.5000e+01  0.0000e+00  0.0000e+00\n","  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n","  0.0000e+00  5.0000e+00  0.0000e+00  0.0000e+00  3.1600e+01  0.0000e+00\n"," -2.0000e+01  0.0000e+00  5.0000e+00  0.0000e+00 -7.6000e+02  1.5000e+01\n","  1.0000e+01  0.0000e+00  0.0000e+00  3.0000e+01 -4.0000e+01 -2.5000e+01\n","  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n","  1.0000e+00  2.6000e+02 -4.3500e+02  0.0000e+00  0.0000e+00 -2.0000e+00\n","  0.0000e+00  0.0000e+00  7.0000e+00  0.0000e+00 -7.0000e+00 -1.1400e+02\n","  0.0000e+00 -1.1400e+02  0.0000e+00  0.0000e+00  1.0000e+00  0.0000e+00\n","  0.0000e+00  1.0000e+00 -4.5000e+01 -4.5000e+01  1.1500e+02 -5.0000e+00\n","  0.0000e+00  0.0000e+00  0.0000e+00  3.1000e+01  0.0000e+00  0.0000e+00\n","  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n","  2.0000e+01  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n","  1.5000e+00  0.0000e+00  0.0000e+00 -3.2361e+04  0.0000e+00  0.0000e+00]--------------\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","Training dataset size: 123265\n","Validation dataset size: 0\n","Testing dataset size: 1246\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2561fe7c8f78>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcont_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/TabCSDI/src/main_model_table_ft.py\u001b[0m in \u001b[0;36mrecover\u001b[0;34m(self, Batch, d_numerical)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_numerical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mL_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"]}]},{"cell_type":"code","source":["with open(\"./data/transformed_columns.pk\", \"rb\") as f:\n","    cont_list, num_cate_list = pickle.load(f)\n","with open(\"./data/encoder.pk\", \"rb\") as f:\n","    encoder = pickle.load(f)\n","\n","temp = torch.zeros(300,224)\n","for i in range(60):\n","  for j in range(5):\n","    temp[i*5+j,:] = model.tokenizer.recover(sample[i,j,:,:].unsqueeze(0), len(cont_list))[0]"],"metadata":{"id":"baOyQ8AWb6xd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"./new_colored_biked_processed.csv\", header=0)\n","\n","masked_saddle = pd.DataFrame(np.repeat(data.values[0:20],5,axis=0))\n","masked_saddle.columns = data.columns\n","masked_saddle[\"Saddle height\"] = pd.NA\n","masked_saddle.to_csv(\"masked_saddle.csv\")\n","\n","masked_fender = pd.DataFrame(np.repeat(data.values[20:40],5,axis=0))\n","masked_fender.columns = data.columns\n","masked_fender[\"Front fender end angle\"] = pd.NA\n","masked_fender.to_csv(\"masked_fender.csv\")\n","\n","masked_rack = pd.DataFrame(np.repeat(data.values[40:60],5,axis=0))\n","masked_rack.columns = data.columns\n","masked_rack[\"Display RACK\"] = pd.NA\n","masked_rack.to_csv(\"masked_rack.csv\")\n","\n","masked_saddle[\"Saddle height\"] = temp[0:100, 10]\n","masked_saddle.to_csv(\"generated_saddle.csv\")\n","\n","masked_fender[\"Front fender end angle\"] = temp[100:200, 119]\n","masked_fender.to_csv(\"generated_fender.csv\")\n"],"metadata":{"id":"8M_a-coXOj79"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["masked_rack[\"Display RACK\"] = temp[200:300, 160] - 1\n","masked_rack.to_csv(\"generated_rack.csv\")"],"metadata":{"id":"UzZHSa-adcEh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Multiple Feature Imputation**"],"metadata":{"id":"3rHhYmp2fB8I"}},{"cell_type":"code","source":["_, _, test_loader = get_dataloader(seed=3047, batch_size=60, nfold=100, missing_ratio=0)\n","\n","with open(\"./column_index.pk\", \"rb\") as f:\n","  column_index = pickle.load(f)\n","\n","for i in column_index:\n","  if i not in [\"Saddle height\", \"Display RACK\", \"Front fender end angle\"] and \"bottle\" not in i:\n","    test_loader.dataset.gt_masks[:,column_index[i]] = False\n","\n","sample, _, _, _, _ = model.evaluate(next(iter(test_loader)), 5)\n","sample = sample.permute(0, 1, 3, 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9NVn-fXVp2Op","outputId":"9f2b19c9-a3eb-44ce-ca57-aff4a77bebb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------Normalized dataset loaded--------\n","Dataset size:124511 entries\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","--------Normalized dataset loaded--------\n","Training dataset size: 123265\n","Validation dataset size: 0\n","Testing dataset size: 1246\n"]}]},{"cell_type":"code","source":["with open(\"./data/transformed_columns.pk\", \"rb\") as f:\n","    cont_list, num_cate_list = pickle.load(f)\n","with open(\"./data/encoder.pk\", \"rb\") as f:\n","        encoder = pickle.load(f)\n","\n","temp = torch.zeros(100,224)\n","for i in range(20):\n","  for j in range(5):\n","    temp[i*5+j,:] = model.tokenizer.recover(sample[i,j,:,:].unsqueeze(0), len(cont_list))[0]"],"metadata":{"id":"3Nwe-rqtsRQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"./new_colored_biked_processed.csv\", header=0)\n","\n","masked_saddle = pd.DataFrame(np.repeat(data.values[0:20],5,axis=0))\n","masked_saddle.columns = data.columns\n","\n","for i in column_index:\n","  if i not in [\"Saddle height\", \"Display RACK\", \"Front fender end angle\"] and \"bottle\" not in i:\n","    masked_saddle[i] = pd.NA\n","\n","masked_saddle.to_csv(\"masked.csv\")\n","\n","for i in column_index:\n","  if i not in [\"Saddle height\", \"Display RACK\", \"Front fender end angle\"] and \"bottle\" not in i:\n","    index = column_index[i]\n","    if index not in cont_list:\n","      masked_saddle[i] = temp[:, index] - 1\n","    else:\n","      masked_saddle[i] = temp[:, index]\n","\n","masked_saddle.to_csv(\"generated.csv\")"],"metadata":{"id":"s6v19dl0tCWw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"OFngflcMeCyt"},"execution_count":null,"outputs":[]}]}